{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build history of ETF shares outstanding and of commodity COTs\n",
    "\n",
    "### For ETF histories:\n",
    "Several companies that manage ETFs provide histores of the NAVs and shares outstanding of their ETF products.  This notebook fetchs that history data from those websites, and assembles a Pandas DataFrame with columns:\n",
    "1. symbol: like SPY or XLE\n",
    "2. date: a datetime.datetime object\n",
    "3. nav: the funds nav for that date\n",
    "4. shares: the shares outstanding at the end of that date\n",
    "5. pc: the percent change of those shares outstanding\n",
    "6. share_diff: the absolute change of those shares outstanding\n",
    "\n",
    "### For COT histories:\n",
    "1. Retrieve data from the CFTC website (www.cft.gov/files);\n",
    "2. Extract data from the most important Commercial and Non Commercial long and short columns;\n",
    "3. Create \"net\" columns for each important category;\n",
    "4. Merge this data with the ETF history data created in the previous steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "def str_to_date(d):\n",
    "    try:\n",
    "        dt = datetime.datetime.strptime(str(d),'%Y-%m-%d')\n",
    "    except:\n",
    "        return None\n",
    "    return dt\n",
    "\n",
    "# Make important folders\n",
    "TEMP_FOLDER = './temp_folder'\n",
    "try:\n",
    "    os.mkdir(TEMP_FOLDER)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(f'{TEMP_FOLDER}/cot')\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir(f'{TEMP_FOLDER}/zip')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## First, decide if you want to re-create the ETF and COT data, or just retrieve the previously saved data DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_ETF_DATA = False\n",
    "CREATE_COT_DATA = False\n",
    "etf_save_path = './etf_cap_hist.csv'\n",
    "cot_save_path = './cot_history.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Retrieve data from iShares\n",
    "Currently, you must physically download each etf history because iShares does not support csv downloads for the commoditiy ETFs.  For other funds, they do.\n",
    "\n",
    "As an example, to process the iShares SLV (silver etf) download:\n",
    "1. Download the file from the url \"https://www.ishares.com/us/products/239855/ishares-silver-trust-fund/1521942788811.ajax?fileType=xls&fileName=iShares-Silver-Trust_fund&dataType=fund\" to a local folder;\n",
    "2. That url is actually an xml file which Microsoft Excel can convert into an xls workbook;\n",
    "3. Open that file in Microsoft Excel;\n",
    "4. Save the \"Historical\" worksheet as a csv with the file name 'etf_history.csv', where etf is something like \"slv\".\n",
    "  * **Make sure you save the file in the folder that contains this jupyter Noteboook**\n",
    "5. Execute the code below.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CREATE_ETF_DATA:\n",
    "    ishares_symbol_list = ['SLV']\n",
    "    ishares_csv_list = [\"./slv_history.csv\"]\n",
    "    df_ishares = None\n",
    "    for i in range(len(ishares_symbol_list)):\n",
    "        url = ishares_csv_list[i]\n",
    "        isym = ishares_symbol_list[i]\n",
    "        df_temp = pd.read_csv(url)\n",
    "        df_temp['symbol'] = isym\n",
    "        if df_ishares is None:\n",
    "            df_ishares = df_temp.copy()\n",
    "        else:\n",
    "            df_ishares = df_ishares.append(df_temp)\n",
    "    df_ishares.head() \n",
    "    def ishares_date(d):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(str(d),'%b %d, %Y')\n",
    "        except:\n",
    "            return None\n",
    "        return dt\n",
    "    df_ishares['date'] = df_ishares['As Of'].apply(ishares_date)\n",
    "    df_ishares = df_ishares.rename(columns = {'NAV per Share':'nav','Shares Outstanding':'shares'})\n",
    "    df_ishares = df_ishares[['symbol','date','nav','shares']]\n",
    "    df_ishares = df_ishares[~df_ishares.date.isnull()].sort_values(['symbol','date'])\n",
    "    df_ishares.index = list(range(len(df_ishares)))\n",
    "    df_ishares.shares = df_ishares.shares.apply(lambda s:float(str(s).replace(',','')))\n",
    "    df_ishares.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Retrieve data from ProFunds using the url from accounts.profunds\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_ETF_DATA:\n",
    "    profunds_symbol_list = ['UCO']\n",
    "    df_profunds = None\n",
    "    for psym in profunds_symbol_list:\n",
    "        url = f\"https://accounts.profunds.com/etfdata/ByFund/{psym}-historical_nav.csv\"\n",
    "        df_temp = pd.read_csv(url)\n",
    "        df_temp['symbol'] = psym\n",
    "        if df_profunds is None:\n",
    "            df_profunds = df_temp.copy()\n",
    "        else:\n",
    "            df_profunds = df_profunds.append(df_temp)\n",
    "    df_profunds.head()\n",
    "    def profunds_date(d):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(str(d),'%m/%d/%Y')\n",
    "        except:\n",
    "            return None\n",
    "        return dt\n",
    "    df_profunds.columns.values,df_profunds.head()\n",
    "    df_profunds['date'] = df_profunds.Date.apply(profunds_date)\n",
    "    df_profunds = df_profunds.rename(columns = {'NAV':'nav','Shares Outstanding (000)':'shares'})\n",
    "    df_profunds = df_profunds[['symbol','date','nav','shares']]\n",
    "    df_profunds = df_profunds[~df_profunds.date.isnull()].sort_values(['symbol','date'])\n",
    "    df_profunds.index = list(range(len(df_profunds)))\n",
    "    df_profunds.shares = df_profunds.shares.apply(lambda s:float(str(s).replace(',','')))\n",
    "    df_profunds.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Retrieve ETF data for multiple ETFs from us.spdrs.com\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if CREATE_ETF_DATA:\n",
    "    spdr_symbol_list = ['GLD','SPY','XLB','XLE','XLF','XLI','XLK','XLP','XLU']\n",
    "    df_spdr = None\n",
    "    for ssym in spdr_symbol_list:\n",
    "        url = f'https://us.spdrs.com/site-content/xls/{ssym}_HistoricalNav.xls?fund={ssym}&docname=Most+Recent+NAV+%2F+NAV+History&onyx_code1=&onyx_code2='\n",
    "    #     df_temp = pd.read_excel('https://us.spdrs.com/site-content/xls/SPY_HistoricalNav.xls?fund=SPY&docname=Most+Recent+NAV+%2F+NAV+History&onyx_code1=&onyx_code2=',skiprows=3)\n",
    "        df_temp = pd.read_excel(url,skiprows=3)\n",
    "        df_temp['symbol'] = ssym\n",
    "        if df_spdr is None:\n",
    "            df_spdr = df_temp.copy()\n",
    "        else:\n",
    "            df_spdr = df_spdr.append(df_temp)\n",
    "    def spdr_date(d):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(str(d),'%d-%b-%Y')\n",
    "        except:\n",
    "            return None\n",
    "        return dt\n",
    "    df_spdr['date'] = df_spdr.Date.apply(spdr_date)\n",
    "    df_spdr = df_spdr.rename(columns = {'Nav':'nav','Shares Outstanding':'shares'})\n",
    "    df_spdr = df_spdr[['symbol','date','nav','shares']]\n",
    "    df_spdr = df_spdr[~df_spdr.date.isnull()].sort_values(['symbol','date'])\n",
    "    df_spdr.index = list(range(len(df_spdr)))\n",
    "    df_spdr.shares = df_spdr.shares.apply(lambda s:float(str(s).replace(',','')))\n",
    "    df_spdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Combine the 3 separate dataframes into one and save it\n",
    "1. Append df_ishares, df_profunds and df_spdr rows together;\n",
    "2. Save the combined DataFrame to ./etf_cap_hist.csv;\n",
    "3. Read that csv back into the DataFrame dff\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if CREATE_ETF_DATA:\n",
    "    df_all = df_ishares.copy()\n",
    "    df_all = df_all.append(df_profunds,ignore_index=True)\n",
    "    df_all = df_all.append(df_spdr,ignore_index=True)\n",
    "    df_all.index = list(range(len(df_all)))\n",
    "    df_all.to_csv(etf_save_path,index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create the following columns:\n",
    "1. pc: shares daily percent change\n",
    "2. share_diff: daily difference from the shares column\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff  = pd.read_csv(etf_save_path)\n",
    "dff['date'] = dff.date.apply(str_to_date)\n",
    "df_fund_flows = dff.copy()\n",
    "df_fund_flows['shares'] = df_fund_flows.shares.apply(lambda s:float(str(s).replace(',','')))\n",
    "df_fund_flows = df_fund_flows.sort_values(['symbol','date'])\n",
    "symbol_list = list(set(df_fund_flows.symbol))\n",
    "df_final = None\n",
    "for sym in symbol_list:\n",
    "    df_this_sym= df_fund_flows[df_fund_flows.symbol==sym]\n",
    "    df_this_sym['pc'] = df_this_sym.shares.pct_change()\n",
    "    df_this_sym['share_diff'] = df_this_sym.shares.diff()\n",
    "    df_this_sym = df_this_sym[df_this_sym.pc.notnull()]\n",
    "    if df_final is None:\n",
    "        df_final = df_this_sym.copy()\n",
    "    else:\n",
    "        df_final = df_final.append(df_this_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create the group_by Dataframes:\n",
    "1. df_pc_gb: shows the min and max per symbol of the pc column;\n",
    "2. df_share_diff_gb: shows the min and max per symbol of the share_diff column.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_beg = datetime.datetime.now() - datetime.timedelta(1000)\n",
    "df_final2 = df_final[(df_final.date>dt_beg)]\n",
    "df_pc = df_final2[['symbol','pc']] \n",
    "df_pc_gb = df_pc.groupby('symbol',as_index=False).agg({'pc':[min,max]})\n",
    "df_pc_gb.columns = ['symbol'] + [ t[0]+ '_' + t[1] for t in df_pc_gb.columns.values[1:]]\n",
    "df_share_diff = df_final2[['symbol','share_diff']] \n",
    "df_share_diff_gb = df_share_diff.groupby('symbol',as_index=False).agg({'share_diff':[min,max]})\n",
    "df_share_diff_gb.columns = ['symbol'] + [ t[0]+ '_' + t[1] for t in df_share_diff_gb.columns.values[1:]]\n",
    "df_pc_gb,df_share_diff_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Create a DataFrame that has the pc of each security in a separate column.\n",
    "This dataframe will make it easy to graph histograms of the pc values\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_all = None\n",
    "for sym in list(set(df_final2.symbol)):\n",
    "    df_temp = df_final2[df_final2.symbol==sym][['date','pc']]\n",
    "    df_temp = df_temp.rename(columns={'pc':sym})\n",
    "    if df_pc_all is None:\n",
    "        df_pc_all = df_temp.copy()\n",
    "    else:\n",
    "        df_pc_all = df_pc_all.merge(df_temp,how='inner',on='date')\n",
    "df_pc_all.index = df_pc_all.date\n",
    "df_pc_all = df_pc_all[list(filter(lambda c:'date' not in c,df_pc_all.columns.values))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc_all.columns.values,df_pc_all.as_matrix().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Graph histograms of the pc column\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pc_all.hist(bins=100,figsize=(20,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show frequency of outlier changes in shares outstanding in GLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djan1 = datetime.datetime(2018,1,1)\n",
    "djan2 = datetime.datetime(2019,2,28)\n",
    "c1 = df_final2.symbol=='GLD'\n",
    "c2 = df_final2.date>=djan1\n",
    "c3 = df_final2.date<=djan2\n",
    "c4 = df_final2.pc >= .009\n",
    "allc = (c1) & (c2) & (c3) & (c4)\n",
    "df_final2[allc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Process CFTC COT Data\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial processing\n",
    "1. Download zip files from www.cft.gov/files;\n",
    "2. Unip the files using the zipfile package;\n",
    "3. Read each csv (usually named Annual.TXT), and merge them into the df_cot DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CREATE_COT_DATA:\n",
    "    year_list = np.linspace(2000,2019,20)\n",
    "#     directory_to_extract_to = TEMP_FOLDER\n",
    "    zip_download_folder = f'TEMP_FOLDER/zip'\n",
    "#     annual_path = f'{zip_download_folder}/Annual.TXT'\n",
    "    df_cot_temp = None\n",
    "    df_cot = None\n",
    "    for y in year_list:\n",
    "        yint = int(y)\n",
    "        url = f\"https://www.cftc.gov/files/dea/history/deacot{yint}.zip\"\n",
    "        path_to_zip_file = f'{zip_download_folder}/dea_fut_xls_{y}.zip'\n",
    "        print(f'retrieving cot zip file from {url}')\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, path_to_zip_file)    \n",
    "        except:\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "            urllib.request.urlretrieve(url, path_to_zip_file)    \n",
    "        zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
    "        zip_ref.extractall(zip_download_folder)\n",
    "        zip_ref.close()\n",
    "        df_cot_temp = pd.read_csv(f'{zip_download_folder}/Annual.TXT')\n",
    "        if df_cot is None:\n",
    "            df_cot = df_cot_temp.copy()\n",
    "        else:\n",
    "            df_cot = df_cot.append(df_cot_temp,ignore_index=True)\n",
    "            df_cot.index = list(range(len(df_cot)))\n",
    "        print(f'processed cot csv file from {url}. Length = {len(df_cot_temp)}')    \n",
    "    df_cot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Make column names easier to process, make main date field a datetime object, and sort the DataFrame\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_COT_DATA:\n",
    "    col_rename_dict = {c:c.replace(' ','_').replace('-','_').replace('(','').replace(')','') for c in df_cot.columns.values}\n",
    "    df_cot2 = df_cot.rename(columns=col_rename_dict)\n",
    "    df_cot2.As_of_Date_in_Form_YYYY_MM_DD = df_cot2.As_of_Date_in_Form_YYYY_MM_DD.apply(str_to_date)\n",
    "    df_cot2 = df_cot2.sort_values(['Market_and_Exchange_Names','As_of_Date_in_Form_YYYY_MM_DD'])\n",
    "    df_cot2.columns.values\n",
    "    df_cot2.to_csv(cot_save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Show important columns for a specific  commodity\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cot2 = pd.read_csv(cot_save_path)\n",
    "df_cot2.As_of_Date_in_Form_YYYY_MM_DD = df_cot2.As_of_Date_in_Form_YYYY_MM_DD.apply(str_to_date)\n",
    "# commod = 'CRUDE OIL, LIGHT SWEET'\n",
    "commod = 'GOLD'\n",
    "cot_beg_date = datetime.datetime.now() - datetime.timedelta(2000)\n",
    "df_commod = df_cot2[df_cot2.Market_and_Exchange_Names.str.contains(commod)][df_cot2.As_of_Date_in_Form_YYYY_MM_DD>=cot_beg_date]\n",
    "basic_cols = ['Market_and_Exchange_Names','As_of_Date_in_Form_YYYY_MM_DD','Open_Interest_All']\n",
    "long_cols = ['Market_and_Exchange_Names','As_of_Date_in_Form_YYYY_MM_DD',\n",
    "            'Noncommercial_Positions_Long_All','Commercial_Positions_Long_All',\n",
    "            'Nonreportable_Positions_Long_All','Traders_Commercial_Long_All',\n",
    "             'Traders_Noncommercial_Long_All','Traders_Total_Reportable_Long_All']\n",
    "short_cols = ['Market_and_Exchange_Names','As_of_Date_in_Form_YYYY_MM_DD',\n",
    "            'Noncommercial_Positions_Short_All','Commercial_Positions_Short_All',\n",
    "            'Nonreportable_Positions_Short_All','Total_Reportable_Positions_Short_All',\n",
    "            'Traders_Commercial_Short_All','Traders_Noncommercial_Short_All',\n",
    "            'Traders_Total_Reportable_Short_All']\n",
    "df_commod_basic = df_commod[basic_cols]\n",
    "df_commod_long = df_commod[long_cols]\n",
    "df_commod_short = df_commod[short_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show basic open interest info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commod_basic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show important \"long\" position info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_commod_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show important \"short\" position info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commod_short.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show important \"net\" position info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def non_comm_net(r):\n",
    "    return float(r.Noncommercial_Positions_Long_All) - float(r.Noncommercial_Positions_Short_All)\n",
    "def comm_net(r):\n",
    "    return float(r.Commercial_Positions_Long_All) - float(r.Commercial_Positions_Short_All)\n",
    "def non_report_net(r):\n",
    "    return float(r.Nonreportable_Positions_Long_All) - float(r.Nonreportable_Positions_Short_All)\n",
    "def traders_comm_net(r):\n",
    "    return float(r.Traders_Commercial_Long_All) - float(r.Traders_Commercial_Short_All)\n",
    "def traders_noncomm_net(r):\n",
    "    return float(r.Traders_Noncommercial_Long_All) - float(r.Traders_Noncommercial_Short_All)\n",
    "\n",
    "df_commod_net = df_commod_long.merge(df_commod_short,how='inner',on=['Market_and_Exchange_Names','As_of_Date_in_Form_YYYY_MM_DD'])\n",
    "df_commod_net['Noncommercial_Positions_Net_All'] = df_commod_net.apply(non_comm_net,axis=1)\n",
    "df_commod_net['Commercial_Positions_Net_All'] = df_commod_net.apply(comm_net,axis=1)\n",
    "df_commod_net['Nonreportable_Positions_Net_All'] = df_commod_net.apply(non_report_net,axis=1)\n",
    "df_commod_net['Traders_Commercial_Net_All'] = df_commod_net.apply(traders_comm_net,axis=1)\n",
    "df_commod_net['Traders_Noncommercial_Net_All'] = df_commod_net.apply(traders_noncomm_net,axis=1)\n",
    "net_cols = ['Market_and_Exchange_Names','As_of_Date_in_Form_YYYY_MM_DD','Noncommercial_Positions_Net_All','Commercial_Positions_Net_All','Nonreportable_Positions_Net_All','Traders_Commercial_Net_All','Traders_Noncommercial_Net_All']\n",
    "df_commod_net = df_commod_net[net_cols]\n",
    "df_commod_net.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Merge ETF shares data with COT data\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose contract and limit dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_symbol = \"GLD\"\n",
    "etf_cond_1 = df_final.symbol==etf_symbol\n",
    "etf_cond_2 = df_final.date>=cot_beg_date\n",
    "etf_cond_all = (etf_cond_1 & etf_cond_2)\n",
    "df_final3 = df_final[etf_cond_all]\n",
    "df_final3.index = list(range(len(df_final3)))\n",
    "df_final3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge with gold cot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql as psql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commod_net2 = df_commod_net.copy()\n",
    "df_commod_net2['next_date'] = df_commod_net2.shift(-1).As_of_Date_in_Form_YYYY_MM_DD\n",
    "df_commod_net2[df_commod_net2.next_date.isnull()] = datetime.datetime.now() + datetime.timedelta(1)\n",
    "\n",
    "q = \"\"\"select * from df_final3\n",
    "join df_commod_net2 on df_final3.date >= df_commod_net2.As_of_Date_in_Form_YYYY_MM_DD \n",
    "     and df_final3.date < df_commod_net2.next_date \n",
    "\"\"\"\n",
    "pysqldf = lambda q: psql.sqldf(q, globals())\n",
    "df_net = pysqldf(q)\n",
    "df_net2 = df_net[['symbol','date','As_of_Date_in_Form_YYYY_MM_DD','nav',\n",
    "                  'shares','pc','share_diff','Noncommercial_Positions_Net_All',\n",
    "                  'Commercial_Positions_Net_All','Nonreportable_Positions_Net_All']]\n",
    "df_net2 = df_net2.rename(columns={\n",
    "    'As_of_Date_in_Form_YYYY_MM_DD':'cot_date',\n",
    "    'Noncommercial_Positions_Net_All':'spec_large','Commercial_Positions_Net_All':'trade',\n",
    "    'Nonreportable_Positions_Net_All':'spec_small'})\n",
    "df_net2['fut_diff'] = df_net2.share_diff/ 1000 \n",
    "df_net3 = df_net2[['symbol','cot_date','fut_diff','spec_large','spec_small']]\n",
    "df_net3.fut_diff = df_net3.fut_diff.astype(float) \n",
    "df_net3.spec_large = df_net3.spec_large.astype(float) \n",
    "df_net3.spec_small = df_net3.spec_small.astype(float) \n",
    "df_net3['spec'] = df_net3.spec_large + df_net3.spec_small\n",
    "f = {'fut_diff':sum,'spec':np.mean,'spec_large':np.mean,'spec_small':np.mean}\n",
    "df_net3_agg = df_net3.groupby(['symbol','cot_date'],as_index=False).agg(f)\n",
    "df_net3_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_per_plot = 40\n",
    "plots = int(len(df_net3_agg)/dates_per_plot) + 1 if len(df_net3_agg) % dates_per_plot > 0 else 0\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=plots, ncols=1)\n",
    "df_net4_agg = df_net3_agg.copy()\n",
    "f = plt.figure()\n",
    "def int_date(d):\n",
    "    return int(str(d)[0:4]+str(d)[5:7]+str(d)[8:10])\n",
    "df_net4_agg.index = df_net4_agg.cot_date.apply(int_date)\n",
    "image_names = []\n",
    "for p in range(plots):\n",
    "    low_row = p * dates_per_plot\n",
    "    high_row = low_row + dates_per_plot\n",
    "    ax = df_net4_agg[['fut_diff','spec',]].iloc[low_row:high_row].plot.bar(figsize=(14,6))\n",
    "    fig = ax.get_figure()\n",
    "    image_name = f\"./temp_folder/cot/cot_net_{p+1}.png\"\n",
    "    fig.savefig(image_name)\n",
    "    image_names.append(image_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge images into 1 vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs    = [ Image.open(i) for i in image_names ]\n",
    "# pick the image which is the smallest, and resize the others to match it (can be arbitrary image shape here)\n",
    "min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n",
    "imgs_comb = np.hstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "\n",
    "# for a vertical stacking it is simple: use vstack\n",
    "# imgs_comb = np.vstack( (np.asarray( i.resize(min_shape) ) for i in imgs ) )\n",
    "imgs_comb = np.vstack( (np.asarray(i) for i in imgs ) )\n",
    "imgs_comb = Image.fromarray( imgs_comb)\n",
    "save_file = f\"./temp_folder/cot/cot_net.png\"\n",
    "imgs_comb.save( save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
